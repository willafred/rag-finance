{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA4265 Assignment 2: RAG Generation\n",
    "\n",
    "With the large availability of news available today from different agencies, it is increasingly difficult for investors to spend time to look through all news articles in order to obtain the answer that they are looking for. \n",
    "\n",
    "Therefore, the goal of this assignment is to create a search engine that summarises key information about the recent stock data in order to have a better context of the stock such that investors can make a more informed decision about the performance of the stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Extraction\n",
    "\n",
    "The following section describes the data extraction process and generation of the labelled dataframe. The tickers used for analysis are that of Apple Inc. (AAPL) and Tesla stocks (TSLA). The data obtained was sourced from Refinitiv Workspace, and the code to extract the dataframes were all copied and pasted from its in-built CodeBook. News headlines were limited to top deals for digital finance, corporate finance, and overall news about the stock itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\refinitiv\\data\\eikon\\__init__.py:15:FutureWarning: The refinitiv.data.eikon module will be removed in future library version v2.0. Please install and use the 'eikon' Python library instead or migrate your code to the Refinitiv/LSEG Data Library\n"
     ]
    }
   ],
   "source": [
    "import refinitiv.data as rd\n",
    "from refinitiv.data.content import news\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import time\n",
    "import warnings\n",
    "import refinitiv.data.eikon as ek\n",
    "from IPython.display import HTML\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<refinitiv.data.session.Definition object at 0x2969ae560e0 {name='workspace'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.open_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>storyId</th>\n",
       "      <th>sourceCode</th>\n",
       "      <th>ric</th>\n",
       "      <th>full_story</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versionCreated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-02-27 13:00:00</th>\n",
       "      <td>RPT-BREAKINGVIEWS-GM illuminates good times be...</td>\n",
       "      <td>urn:newsml:reuters.com:20250227:nL3N3PH1P3:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>(The author is a Reuters Breakingviews columni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-25 12:52:47</th>\n",
       "      <td>Tesla to acquire parts of insolvent German par...</td>\n",
       "      <td>urn:newsml:reuters.com:20250225:nL5N3PG0Y9:7</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>* \\n      Acquisition includes 300 staff, excl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-25 01:10:32</th>\n",
       "      <td>RPT-BREAKINGVIEWS-Nissan offers suitors daunti...</td>\n",
       "      <td>urn:newsml:reuters.com:20250225:nL3N3PG036:3</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>(The author is a Reuters Breakingviews columni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-24 12:00:00</th>\n",
       "      <td>RPT-BREAKINGVIEWS-Nissan offers suitors daunti...</td>\n",
       "      <td>urn:newsml:reuters.com:20250224:nL3N3PF0DU:4</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>(The author is a Reuters Breakingviews columni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-21 13:55:05</th>\n",
       "      <td>UPDATE 6-Japan seeks Tesla investment in Nissa...</td>\n",
       "      <td>urn:newsml:reuters.com:20250221:nL3N3PC0GR:2</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>* \\n      Japanese group draws up plans for Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-14 15:03:57</th>\n",
       "      <td>US STOCKS-Wall St subdued as markets await tar...</td>\n",
       "      <td>urn:newsml:reuters.com:20250214:nL4N3P515Y:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>AAPL.O</td>\n",
       "      <td>(For a Reuters live blog on U.S., UK and Europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-14 14:04:36</th>\n",
       "      <td>US STOCKS-Wall St set for subdued open as mark...</td>\n",
       "      <td>urn:newsml:reuters.com:20250214:nL4N3P512O:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>AAPL.O</td>\n",
       "      <td>(For a Reuters live blog on U.S., UK and Europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-14 12:31:59</th>\n",
       "      <td>US STOCKS-Futures slip as markets await tariff...</td>\n",
       "      <td>urn:newsml:reuters.com:20250214:nL4N3P50XC:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>AAPL.O</td>\n",
       "      <td>(For a Reuters live blog on U.S., UK and Europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-14 11:50:37</th>\n",
       "      <td>REFILE-FACTBOX-China's AI firms take spotlight...</td>\n",
       "      <td>urn:newsml:reuters.com:20250214:nL1N3P50BI:1</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>AAPL.O</td>\n",
       "      <td>(Refiles to correct transposed letters in Doub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-14 08:57:56</th>\n",
       "      <td>China tech stocks cap best rally in over two y...</td>\n",
       "      <td>urn:newsml:reuters.com:20250214:nL1N3P50AJ:2</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>AAPL.O</td>\n",
       "      <td>(Updates closing prices, adds more details and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              headline  \\\n",
       "versionCreated                                                           \n",
       "2025-02-27 13:00:00  RPT-BREAKINGVIEWS-GM illuminates good times be...   \n",
       "2025-02-25 12:52:47  Tesla to acquire parts of insolvent German par...   \n",
       "2025-02-25 01:10:32  RPT-BREAKINGVIEWS-Nissan offers suitors daunti...   \n",
       "2025-02-24 12:00:00  RPT-BREAKINGVIEWS-Nissan offers suitors daunti...   \n",
       "2025-02-21 13:55:05  UPDATE 6-Japan seeks Tesla investment in Nissa...   \n",
       "...                                                                ...   \n",
       "2025-02-14 15:03:57  US STOCKS-Wall St subdued as markets await tar...   \n",
       "2025-02-14 14:04:36  US STOCKS-Wall St set for subdued open as mark...   \n",
       "2025-02-14 12:31:59  US STOCKS-Futures slip as markets await tariff...   \n",
       "2025-02-14 11:50:37  REFILE-FACTBOX-China's AI firms take spotlight...   \n",
       "2025-02-14 08:57:56  China tech stocks cap best rally in over two y...   \n",
       "\n",
       "                                                          storyId sourceCode  \\\n",
       "versionCreated                                                                 \n",
       "2025-02-27 13:00:00  urn:newsml:reuters.com:20250227:nL3N3PH1P3:5    NS:RTRS   \n",
       "2025-02-25 12:52:47  urn:newsml:reuters.com:20250225:nL5N3PG0Y9:7    NS:RTRS   \n",
       "2025-02-25 01:10:32  urn:newsml:reuters.com:20250225:nL3N3PG036:3    NS:RTRS   \n",
       "2025-02-24 12:00:00  urn:newsml:reuters.com:20250224:nL3N3PF0DU:4    NS:RTRS   \n",
       "2025-02-21 13:55:05  urn:newsml:reuters.com:20250221:nL3N3PC0GR:2    NS:RTRS   \n",
       "...                                                           ...        ...   \n",
       "2025-02-14 15:03:57  urn:newsml:reuters.com:20250214:nL4N3P515Y:5    NS:RTRS   \n",
       "2025-02-14 14:04:36  urn:newsml:reuters.com:20250214:nL4N3P512O:5    NS:RTRS   \n",
       "2025-02-14 12:31:59  urn:newsml:reuters.com:20250214:nL4N3P50XC:5    NS:RTRS   \n",
       "2025-02-14 11:50:37  urn:newsml:reuters.com:20250214:nL1N3P50BI:1    NS:RTRS   \n",
       "2025-02-14 08:57:56  urn:newsml:reuters.com:20250214:nL1N3P50AJ:2    NS:RTRS   \n",
       "\n",
       "                        ric                                         full_story  \n",
       "versionCreated                                                                  \n",
       "2025-02-27 13:00:00  TSLA.O  (The author is a Reuters Breakingviews columni...  \n",
       "2025-02-25 12:52:47  TSLA.O  * \\n      Acquisition includes 300 staff, excl...  \n",
       "2025-02-25 01:10:32  TSLA.O  (The author is a Reuters Breakingviews columni...  \n",
       "2025-02-24 12:00:00  TSLA.O  (The author is a Reuters Breakingviews columni...  \n",
       "2025-02-21 13:55:05  TSLA.O  * \\n      Japanese group draws up plans for Te...  \n",
       "...                     ...                                                ...  \n",
       "2025-02-14 15:03:57  AAPL.O  (For a Reuters live blog on U.S., UK and Europ...  \n",
       "2025-02-14 14:04:36  AAPL.O  (For a Reuters live blog on U.S., UK and Europ...  \n",
       "2025-02-14 12:31:59  AAPL.O  (For a Reuters live blog on U.S., UK and Europ...  \n",
       "2025-02-14 11:50:37  AAPL.O  (Refiles to correct transposed letters in Doub...  \n",
       "2025-02-14 08:57:56  AAPL.O  (Updates closing prices, adds more details and...  \n",
       "\n",
       "[262 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_full_story(story_id):\n",
    "    try:\n",
    "        story = rd.news.get_story(story_id, format=rd.news.Format.TEXT)\n",
    "        return story if story else \"Story not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {story_id}: {e}\")\n",
    "        return \"Error retrieving story\"\n",
    "\n",
    "dNow = datetime.now().date()\n",
    "maxenddate = dNow - timedelta(days=90) #upto months=15\n",
    "compNews = pd.DataFrame()\n",
    "riclist = ['TSLA.O','AAPL.O'] # can also use Peers, Customers, Suppliers, Monitor, Portfolio to build universe\n",
    "\n",
    "for ric in riclist:\n",
    "    try:\n",
    "        cHeadlines = rd.news.get_headlines(\"R:\" + ric + \" AND Language:LEN AND Source:RTRS AND (Topic:TOP/DEALS OR Topic:TOP/DIGFIN)\", \n",
    "                                           start= str(dNow), \n",
    "                                           end = str(maxenddate), count = 100)\n",
    "        cHeadlines['ric'] = ric\n",
    "        # Corporate Finance: TOP/DEALS, Broker Research / Recommendation: RCH\n",
    "        if len(compNews):\n",
    "            compNews = pd.concat([compNews,cHeadlines])\n",
    "        else:\n",
    "            compNews = cHeadlines\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Apply to all rows\n",
    "compNews[\"full_story\"] = compNews[\"storyId\"].apply(fetch_full_story)\n",
    "\n",
    "compNews2 = pd.DataFrame()\n",
    "riclist = ['TSLA.O','AAPL.O'] # can also use Peers, Customers, Suppliers, Monitor, Portfolio to build universe\n",
    "\n",
    "for ric in riclist:\n",
    "    try:\n",
    "        cHeadlines = rd.news.get_headlines(\"R:\" + ric + \" AND Language:LEN AND Source:RTRS AND (Topic:TOPALL)\", \n",
    "                                           start= str(dNow), \n",
    "                                           end = str(maxenddate), count = 100)\n",
    "        cHeadlines['ric'] = ric\n",
    "        # Corporate Finance: TOP/DEALS, Broker Research / Recommendation: RCH\n",
    "        if len(compNews):\n",
    "            compNews2 = pd.concat([compNews2,cHeadlines])\n",
    "        else:\n",
    "            compNews2 = cHeadlines\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "compNews2[\"full_story\"] = compNews2[\"storyId\"].apply(fetch_full_story)\n",
    "combined_df = pd.concat([compNews, compNews2], axis = 0)\n",
    "combined_df.to_csv('combined_news_updated.csv')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building of RAG Model\n",
    "\n",
    "The RAG model was built with the help of 3 factors, all of which will be dealt with in greater depth below:\n",
    "\n",
    "### Feature 1: Chunking of Documents\n",
    "\n",
    "To facilitate the separation of documents into distinct chunks, RecursiveTextSplitter function was utilised, with an overlap of 100 characters so as to ensure the preservation of context between chunks. Therefore, this enables better understanding of each chunk.\n",
    "\n",
    "### Feature 2: Sentence Embeddings\n",
    "\n",
    "To assign texts to numerical vectors for the machines to process the text, a sentence transformer model was selected to help in semantic similarity search.\n",
    "\n",
    "### Feature 3: Vector Store\n",
    "\n",
    "The use of Facebook Artificial Intelligence Similarity Search (FAISS) was used to store the embeddings for quick retrieval.\n",
    "\n",
    "### Feature 4: Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m combined_news_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_news_updated.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m tsla_news \u001b[38;5;241m=\u001b[39m combined_news_df[combined_news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mric\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSLA.O\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m aapl_news \u001b[38;5;241m=\u001b[39m combined_news_df[combined_news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mric\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL.O\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "combined_news_df = pd.read_csv(\"combined_news_updated.csv\")\n",
    "tsla_news = combined_news_df[combined_news_df['ric'] == 'TSLA.O']\n",
    "aapl_news = combined_news_df[combined_news_df['ric'] == 'AAPL.O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>versionCreated</th>\n",
       "      <th>headline</th>\n",
       "      <th>storyId</th>\n",
       "      <th>sourceCode</th>\n",
       "      <th>ric</th>\n",
       "      <th>full_story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-27 13:00:00</td>\n",
       "      <td>RPT-BREAKINGVIEWS-GM illuminates good times be...</td>\n",
       "      <td>urn:newsml:reuters.com:20250227:nL3N3PH1P3:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>(The author is a Reuters Breakingviews columni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-25 12:52:47</td>\n",
       "      <td>Tesla to acquire parts of insolvent German par...</td>\n",
       "      <td>urn:newsml:reuters.com:20250225:nL5N3PG0Y9:7</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>* \\n      Acquisition includes 300 staff, excl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-25 01:10:32</td>\n",
       "      <td>RPT-BREAKINGVIEWS-Nissan offers suitors daunti...</td>\n",
       "      <td>urn:newsml:reuters.com:20250225:nL3N3PG036:3</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>(The author is a Reuters Breakingviews columni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-24 12:00:00</td>\n",
       "      <td>RPT-BREAKINGVIEWS-Nissan offers suitors daunti...</td>\n",
       "      <td>urn:newsml:reuters.com:20250224:nL3N3PF0DU:4</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>(The author is a Reuters Breakingviews columni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-21 13:55:05</td>\n",
       "      <td>UPDATE 6-Japan seeks Tesla investment in Nissa...</td>\n",
       "      <td>urn:newsml:reuters.com:20250221:nL3N3PC0GR:2</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>* \\n      Japanese group draws up plans for Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2025-02-18 10:56:15</td>\n",
       "      <td>Netherlands to build 1.4 GW battery storage fa...</td>\n",
       "      <td>urn:newsml:reuters.com:20250218:nL6N3P90C3:2</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>AMSTERDAM, Feb 18 - Dutch energy storage firm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2025-02-18 06:29:17</td>\n",
       "      <td>Tesla steps up India hiring after Musk-Modi me...</td>\n",
       "      <td>urn:newsml:reuters.com:20250218:nL3N3P90AQ:1</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>Feb 18 (Reuters) - Elon Musk's Tesla &lt;TSLA.O&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2025-02-18 04:44:51</td>\n",
       "      <td>Tesla begins mass production of revamped Model...</td>\n",
       "      <td>urn:newsml:reuters.com:20250218:nP8N3O50DP:2</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>BEIJING, Feb 18 (Reuters) - U.S. automaker Tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2025-02-18 03:27:45</td>\n",
       "      <td>Complaints targeting BYD flood Chinese consume...</td>\n",
       "      <td>urn:newsml:reuters.com:20250218:nL3N3P80MF:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>BEIJING, Feb 18 (Reuters) - Complaints about B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2025-02-18 02:21:29</td>\n",
       "      <td>FAA fires fewer than 400 workers, transportati...</td>\n",
       "      <td>urn:newsml:reuters.com:20250218:nL2N3P9010:5</td>\n",
       "      <td>NS:RTRS</td>\n",
       "      <td>TSLA.O</td>\n",
       "      <td>By Valerie Volcovici and David Shepardson\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          versionCreated                                           headline  \\\n",
       "0    2025-02-27 13:00:00  RPT-BREAKINGVIEWS-GM illuminates good times be...   \n",
       "1    2025-02-25 12:52:47  Tesla to acquire parts of insolvent German par...   \n",
       "2    2025-02-25 01:10:32  RPT-BREAKINGVIEWS-Nissan offers suitors daunti...   \n",
       "3    2025-02-24 12:00:00  RPT-BREAKINGVIEWS-Nissan offers suitors daunti...   \n",
       "4    2025-02-21 13:55:05  UPDATE 6-Japan seeks Tesla investment in Nissa...   \n",
       "..                   ...                                                ...   \n",
       "157  2025-02-18 10:56:15  Netherlands to build 1.4 GW battery storage fa...   \n",
       "158  2025-02-18 06:29:17  Tesla steps up India hiring after Musk-Modi me...   \n",
       "159  2025-02-18 04:44:51  Tesla begins mass production of revamped Model...   \n",
       "160  2025-02-18 03:27:45  Complaints targeting BYD flood Chinese consume...   \n",
       "161  2025-02-18 02:21:29  FAA fires fewer than 400 workers, transportati...   \n",
       "\n",
       "                                          storyId sourceCode     ric  \\\n",
       "0    urn:newsml:reuters.com:20250227:nL3N3PH1P3:5    NS:RTRS  TSLA.O   \n",
       "1    urn:newsml:reuters.com:20250225:nL5N3PG0Y9:7    NS:RTRS  TSLA.O   \n",
       "2    urn:newsml:reuters.com:20250225:nL3N3PG036:3    NS:RTRS  TSLA.O   \n",
       "3    urn:newsml:reuters.com:20250224:nL3N3PF0DU:4    NS:RTRS  TSLA.O   \n",
       "4    urn:newsml:reuters.com:20250221:nL3N3PC0GR:2    NS:RTRS  TSLA.O   \n",
       "..                                            ...        ...     ...   \n",
       "157  urn:newsml:reuters.com:20250218:nL6N3P90C3:2    NS:RTRS  TSLA.O   \n",
       "158  urn:newsml:reuters.com:20250218:nL3N3P90AQ:1    NS:RTRS  TSLA.O   \n",
       "159  urn:newsml:reuters.com:20250218:nP8N3O50DP:2    NS:RTRS  TSLA.O   \n",
       "160  urn:newsml:reuters.com:20250218:nL3N3P80MF:5    NS:RTRS  TSLA.O   \n",
       "161  urn:newsml:reuters.com:20250218:nL2N3P9010:5    NS:RTRS  TSLA.O   \n",
       "\n",
       "                                            full_story  \n",
       "0    (The author is a Reuters Breakingviews columni...  \n",
       "1    * \\n      Acquisition includes 300 staff, excl...  \n",
       "2    (The author is a Reuters Breakingviews columni...  \n",
       "3    (The author is a Reuters Breakingviews columni...  \n",
       "4    * \\n      Japanese group draws up plans for Te...  \n",
       "..                                                 ...  \n",
       "157  AMSTERDAM, Feb 18 - Dutch energy storage firm ...  \n",
       "158  Feb 18 (Reuters) - Elon Musk's Tesla <TSLA.O> ...  \n",
       "159  BEIJING, Feb 18 (Reuters) - U.S. automaker Tes...  \n",
       "160  BEIJING, Feb 18 (Reuters) - Complaints about B...  \n",
       "161  By Valerie Volcovici and David Shepardson\\n   ...  \n",
       "\n",
       "[134 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsla_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to documents\n",
    "documents = [\n",
    "    Document(page_content=row['full_story'],\n",
    "             date=row['versionCreated']) \n",
    "    for _, row in tsla_news.iterrows()\n",
    "]\n",
    "\n",
    "# Chunk documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize embedding model for documents and queries\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "# Create FAISS vector database\n",
    "vector_db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Use a generative model for text generation\n",
    "llm_model = \"EleutherAI/gpt-neo-2.7B\"  # You can try other models as needed\n",
    "# llm_model = \"bigscience/bloom-560m\"\n",
    "llm_pipeline = pipeline(\"text-generation\", model=llm_model, device=0 if torch.cuda.is_available() else -1, \n",
    "                        max_new_tokens=100)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Create the RetrievalQA chain, passing in the LLM and vector database retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=vector_db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    \"\"\"Function to ask a question using RAG model with extracted context chunks.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vector_db.similarity_search(query, k=3)\n",
    "    \n",
    "    # Extract the context chunks from the retrieved documents\n",
    "    prompt_context = [doc.page_content for doc in retrieved_docs]\n",
    "    # return prompt_context\n",
    "    # Prepare the prompt\n",
    "    context_str = \"\\n\\n\".join(prompt_context)\n",
    "    prompt = f\"\"\"\n",
    "        You are an AI system. Below are relevant news articles with potential relevance:\n",
    "        {context_str}\n",
    "\n",
    "        Based on these excerpts, if the information is insufficient, say \"I do not have enough information.\" Otherwise, answer the following:\n",
    "        \n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\".strip()\n",
    "    def remove_consecutive_duplicates(text: str):\n",
    "        # Split the text into sentences using regex\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        \n",
    "        # Create a list to store non-duplicate sentences\n",
    "        unique_sentences = []\n",
    "        \n",
    "        # Iterate over the sentences and add them to unique_sentences if not a duplicate\n",
    "        for i in range(len(sentences)):\n",
    "            current_sentence = sentences[i].strip()\n",
    "            \n",
    "            # If it's the first sentence or not a duplicate of the previous one, keep it\n",
    "            if i == 0 or current_sentence != sentences[i - 1].strip():\n",
    "                unique_sentences.append(current_sentence)\n",
    "        \n",
    "        # Join the unique sentences back into a single text\n",
    "        return ' '.join(unique_sentences)\n",
    "    \n",
    "    # Generate the model's response\n",
    "    response = llm_pipeline(prompt)[0]['generated_text']\n",
    "    # Find where the 'Question:' part starts\n",
    "    first_qn_pos = response.lower().find('question:')\n",
    "    \n",
    "    answer_start = response.lower().find('answer:')\n",
    "    \n",
    "    # If 'Answer:' is found, slice the text from there\n",
    "    if answer_start != -1:\n",
    "        answer = response[answer_start + len('answer:'):].strip()  # Extract everything after \"Answer:\"\n",
    "        new_answer = remove_consecutive_duplicates(answer)\n",
    "    else:\n",
    "        answer = \"No answer found.\"\n",
    "    \n",
    "    # answer_end = response.lower().find('a:', qn_start)\n",
    "    # answer_end = response.lower().find('question:', first_qn_pos + len('question:'))\n",
    "    \n",
    "    # # If 'Question:' and 'A:' are found, return the text between them\n",
    "    # if first_qn_pos != -1 and answer_end != -1:\n",
    "    #     answer = response[first_qn_pos:answer_end].strip()\n",
    "    # elif first_qn_pos != -1:\n",
    "    #     # If only 'Question:' is found, return everything from 'Question:' onward\n",
    "    #     answer = response[first_qn_pos:].strip()\n",
    "    # else:\n",
    "    #     # If no 'Answer:' part is found, return the whole generated text\n",
    "    #     answer = response.strip()\n",
    "    final_response = f\"Question:{query}\\nAnswer:{new_answer}\"\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are some risks associated with Tesla lately?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[93], line 40\u001b[0m, in \u001b[0;36mask_question\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_sentences)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Find where the 'Question:' part starts\u001b[39;00m\n\u001b[0;32m     42\u001b[0m first_qn_pos \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\base.py:1268\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         )\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\base.py:1275\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1274\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1275\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1276\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2045\u001b[0m     )\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2049\u001b[0m         input_ids,\n\u001b[0;32m   2050\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2051\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2052\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2053\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2054\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2055\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2056\u001b[0m     )\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2068\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:3008\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3005\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3007\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:1043\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1043\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1059\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:806\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    795\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    796\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    797\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    803\u001b[0m         cache_position,\n\u001b[0;32m    804\u001b[0m     )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:529\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    527\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    528\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 529\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:484\u001b[0m, in \u001b[0;36mGPTNeoMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 484\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m    486\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What are some risks associated with Tesla lately?\"\n",
    "response = ask_question(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The company has a market share of about 50% in the U.S. and Europe. The company has a market share of about 50% in the U.S. and Europe. Claire.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_consecutive_duplicates(text: str):\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())  # Split on punctuation followed by space\n",
    "    \n",
    "    # Create a list to store non-duplicate sentences\n",
    "    unique_sentences = []\n",
    "    \n",
    "    # Iterate over the sentences and add them to unique_sentences if not a duplicate\n",
    "    for i in range(len(sentences)):\n",
    "        current_sentence = sentences[i].strip()\n",
    "        \n",
    "        # If it's the first sentence or not a duplicate of the previous one, keep it\n",
    "        if i == 0 or current_sentence.lower() != sentences[i - 1].strip().lower():\n",
    "            unique_sentences.append(current_sentence)\n",
    "    \n",
    "    # Join the unique sentences back into a single text\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "text = \"The company has a market share of about 50% in the U.S. and Europe. The company has a market share of about 50% in the U.S. and Europe. Claire.\"\n",
    "remove_consecutive_duplicates(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# import torch\n",
    "# # from transformers import HuggingFacePipeline\n",
    "\n",
    "# # Set up the text generation pipeline\n",
    "# llm_model = \"bigscience/bloom-560m\"\n",
    "# llm_pipeline = pipeline(\"text-generation\", model=llm_model, device=0 if torch.cuda.is_available() else -1, \n",
    "#                         max_new_tokens=100)\n",
    "# llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# # Function to ask a question given context\n",
    "# def ask_question_with_context(context: str, question: str):\n",
    "#     # Format the prompt with context and question\n",
    "#     prompt = f\"You are an AI bot who is given the following:\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "#     response = llm(prompt)\n",
    "    \n",
    "#     # Find where the 'Question:' part starts\n",
    "#     qn_start = response.lower().find('question:')\n",
    "    \n",
    "#     # Find where the 'A:' part starts (indicating the end of the answer)\n",
    "#     answer_end = response.lower().find('a:', qn_start)\n",
    "    \n",
    "#     # If 'Question:' and 'A:' are found, return the text between them\n",
    "#     if qn_start != -1 and answer_end != -1:\n",
    "#         answer = response[qn_start:answer_end].strip()\n",
    "#     elif qn_start != -1:\n",
    "#         # If only 'Question:' is found, return everything from 'Question:' onward\n",
    "#         answer = response[qn_start:].strip()\n",
    "#     else:\n",
    "#         # If no 'Answer:' part is found, return the whole generated text\n",
    "#         answer = response.strip()\n",
    "    \n",
    "#     return answer\n",
    "\n",
    "# # Example context and question\n",
    "# context = \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\n",
    "# question = \"Where is Eiffel Tower?\"\n",
    "\n",
    "# # Ask the question based on the context\n",
    "# answer = ask_question_with_context(context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is Eiffel Tower?\n",
      "Answer: The Eiffel Tower is located in the city of Paris, France. The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel, whose company designed and built the tower.\n"
     ]
    }
   ],
   "source": [
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The position of 'Q' in 'Question:' is: 74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Question: Where is Eiffel Tower?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def find_question_q(text: str):\n",
    "#     # Find the position of 'Q' in 'Question:'\n",
    "#     q_position = text.lower().find('question:')\n",
    "    \n",
    "#     # If 'Question:' is found, return the index of the first 'Q'\n",
    "#     if q_position != -1:\n",
    "#         return q_position  # Returns the index of 'Q' in 'Question:'\n",
    "#     else:\n",
    "#         return None  # Return None if 'Question:' is not found\n",
    "\n",
    "# # Example string\n",
    "# example_text = \"You are an AI bot who is given the following:\\nContext: Some context here\\n\\nQuestion: Where is Eiffel Tower?\"\n",
    "\n",
    "# # Find the position of the 'Q' in 'Question:'\n",
    "# q_index = find_question_q(example_text)\n",
    "\n",
    "# # Print the result\n",
    "# print(f\"The position of 'Q' in 'Question:' is: {q_index}\")\n",
    "\n",
    "# example_text[q_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"python-doctr[torch,viz,html,contrib]\"  \n",
    "# !pip install onnx==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_google_vertexai import (\n",
    "    ChatVertexAI,\n",
    "    VectorSearchVectorStore,\n",
    "    VertexAI,\n",
    "    VertexAIEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from google.cloud import aiplatform\n",
    "import fitz  # pymupdf\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"handy-bonbon-453100-e8\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-east4\"  # @param {type:\"string\"}\n",
    "\n",
    "# For Vector Search Staging\n",
    "GCS_BUCKET = \"gen_ai_bucket_129395\"  # @param {type:\"string\"}\n",
    "GCS_BUCKET_URI = f\"gs://{GCS_BUCKET}\"\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=GCS_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=GCS_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "GEMINI_OUTPUT_TOKEN_LIMIT = 8192\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-004\"\n",
    "EMBEDDING_TOKEN_LIMIT = 2048\n",
    "\n",
    "TOKEN_LIMIT = min(GEMINI_OUTPUT_TOKEN_LIMIT, EMBEDDING_TOKEN_LIMIT)\n",
    "\n",
    "# model = VertexAI(\n",
    "#     temperature=0, \n",
    "#     model_name=MODEL_NAME, \n",
    "#     max_output_tokens=TOKEN_LIMIT\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_used = [\"aapl\", \"amzn\", \"ba\", \"brka\", \"googl\", \"gs\", \"jnj\", \"jpm\", \"ko\", \"mcd\", \n",
    "               \"meta\", \"ms\", \"msft\", \"nee\", \"nvda\", \"pfe\", \"pg\", \"tsla\", \"v\", \"xom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdf2image import convert_from_path\n",
    "# images = convert_from_path(\"tesla-stock-report.pdf\", poppler_path=r\"C:\\Users\\wjlwi\\Downloads\\poppler-24.08.0\\Library\\bin\")\n",
    "# for i, img in enumerate(images):\n",
    "#     img.save(f'{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_file_name = \"google-10k-sample-14pages.pdf\"\n",
    "# pdf_folder_path = \"data/\"\n",
    "# # Extract images, tables, and chunk text from a PDF file.\n",
    "# raw_pdf_elements = partition_pdf(\n",
    "#     filename=pdf_file_name,\n",
    "#     extract_images_in_pdf=False,\n",
    "#     infer_table_structure=True,\n",
    "#     chunking_strategy=\"by_title\",\n",
    "#     max_characters=4000,\n",
    "#     new_after_n_chars=3800,\n",
    "#     combine_text_under_n_chars=2000,\n",
    "#     image_output_dir_path=pdf_folder_path,\n",
    "# )\n",
    "\n",
    "# # Categorize extracted elements from a PDF into tables and texts.\n",
    "# tables = []\n",
    "# texts = []\n",
    "# for element in raw_pdf_elements:\n",
    "#     if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "#         tables.append(str(element))\n",
    "#     elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "#         texts.append(str(element))\n",
    "\n",
    "# # Optional: Enforce a specific token size for texts\n",
    "# text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=10000, chunk_overlap=0\n",
    "# )\n",
    "# joined_texts = \" \".join(texts)\n",
    "# texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_text_summaries(\n",
    "    texts: list[str], summarize_texts: bool = False\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Summarise the issues stemming for the report provided. The report is as shown: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "    empty_response = RunnableLambda(\n",
    "        lambda x: AIMessage(content=\"Error processing document\")\n",
    "    )\n",
    "    # Text summary chain\n",
    "    model = VertexAI(\n",
    "        temperature=0, model_name=MODEL_NAME, max_output_tokens=TOKEN_LIMIT\n",
    "    ).with_fallbacks([empty_response])\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    # if texts:\n",
    "    #     if summarize_texts:\n",
    "    #         text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    #     else:\n",
    "    #         text_summaries = texts\n",
    "    if texts:\n",
    "        for i in range(len(texts)):\n",
    "            text = texts[i]\n",
    "            if summarize_texts:\n",
    "                # Summarize the current text chunk\n",
    "                summary = summarize_chain.invoke({\"element\": text})\n",
    "                text_summaries.append(summary)\n",
    "            else:\n",
    "                text_summaries.append(text)\n",
    "            print(f\"Chunk {i} summarised, {len(texts)-i} remaining for this stock\")\n",
    "            # Wait for 1 minute after every 3 chunks\n",
    "            if (i + 1) % 4 == 0 and i != len(texts) - 1:\n",
    "                print(\"Waiting for 1 minute before processing the next 4 chunks...\")\n",
    "                time.sleep(60)  # Delay for 1 minute after every 3 chunks\n",
    "    print(\"Summarised!\")\n",
    "    return text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"handy-bonbon-453100-e8\"  # @param {type:\"string\"}\n",
    "# LOCATION = \"us-central2\"  # @param {type:\"string\"}\n",
    "\n",
    "# 'asia-northeast1', 'us-west3', 'northamerica-northeast1', 'europe-west9', 'asia-northeast3', 'europe-west8', 'europe-west12', \n",
    "# 'africa-south1', 'us-east5', 'asia-south1', 'asia-southeast1', 'asia-east1', 'europe-west4', 'europe-west3', 'northamerica-northeast2', \n",
    "# 'us-west4', 'me-central2', 'us-central1', 'australia-southeast1', 'europe-central2', 'europe-north1', 'me-central1', 'europe-west1', 'us-west1', \n",
    "# 'us-west2', 'asia-east2', 'us-east1', 'me-west1', 'asia-northeast2', 'southamerica-west1', 'australia-southeast2', 'europe-west2', 'us-south1', 'global', \n",
    "# 'asia-southeast2', 'southamerica-east1', 'us-east4', 'europe-west6', 'europe-southwest1'\n",
    "\n",
    "# # For Vector Search Staging\n",
    "# GCS_BUCKET = \"gen_ai_bucket_129395\"  # @param {type:\"string\"}\n",
    "# GCS_BUCKET_URI = f\"gs://{GCS_BUCKET}\"\n",
    "# aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=GCS_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1044, which is longer than the specified 1000\n",
      "Created a chunk of size 1192, which is longer than the specified 1000\n",
      "Created a chunk of size 1049, which is longer than the specified 1000\n",
      "Created a chunk of size 1941, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "aapl Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1176, which is longer than the specified 1000\n",
      "Created a chunk of size 1188, which is longer than the specified 1000\n",
      "Created a chunk of size 1026, which is longer than the specified 1000\n",
      "Created a chunk of size 1892, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "amzn Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1182, which is longer than the specified 1000\n",
      "Created a chunk of size 1240, which is longer than the specified 1000\n",
      "Created a chunk of size 1030, which is longer than the specified 1000\n",
      "Created a chunk of size 1888, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "ba Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1214, which is longer than the specified 1000\n",
      "Created a chunk of size 1060, which is longer than the specified 1000\n",
      "Created a chunk of size 1905, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "brka Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1210, which is longer than the specified 1000\n",
      "Created a chunk of size 1201, which is longer than the specified 1000\n",
      "Created a chunk of size 1032, which is longer than the specified 1000\n",
      "Created a chunk of size 1016, which is longer than the specified 1000\n",
      "Created a chunk of size 1892, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "googl Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1069, which is longer than the specified 1000\n",
      "Created a chunk of size 1201, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1952, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "gs Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1085, which is longer than the specified 1000\n",
      "Created a chunk of size 1187, which is longer than the specified 1000\n",
      "Created a chunk of size 1078, which is longer than the specified 1000\n",
      "Created a chunk of size 1939, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "jnj Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1183, which is longer than the specified 1000\n",
      "Created a chunk of size 1049, which is longer than the specified 1000\n",
      "Created a chunk of size 1939, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "jpm Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1072, which is longer than the specified 1000\n",
      "Created a chunk of size 1203, which is longer than the specified 1000\n",
      "Created a chunk of size 1059, which is longer than the specified 1000\n",
      "Created a chunk of size 1941, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "ko Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n",
      "Created a chunk of size 1196, which is longer than the specified 1000\n",
      "Created a chunk of size 1047, which is longer than the specified 1000\n",
      "Created a chunk of size 1944, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "mcd Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1060, which is longer than the specified 1000\n",
      "Created a chunk of size 1188, which is longer than the specified 1000\n",
      "Created a chunk of size 1051, which is longer than the specified 1000\n",
      "Created a chunk of size 1937, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "meta Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1229, which is longer than the specified 1000\n",
      "Created a chunk of size 1198, which is longer than the specified 1000\n",
      "Created a chunk of size 1037, which is longer than the specified 1000\n",
      "Created a chunk of size 1904, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "ms Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1033, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1047, which is longer than the specified 1000\n",
      "Created a chunk of size 1936, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "msft Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1213, which is longer than the specified 1000\n",
      "Created a chunk of size 1197, which is longer than the specified 1000\n",
      "Created a chunk of size 1014, which is longer than the specified 1000\n",
      "Created a chunk of size 1893, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "nee Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1216, which is longer than the specified 1000\n",
      "Created a chunk of size 1205, which is longer than the specified 1000\n",
      "Created a chunk of size 1022, which is longer than the specified 1000\n",
      "Created a chunk of size 1891, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "nvda Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1237, which is longer than the specified 1000\n",
      "Created a chunk of size 1194, which is longer than the specified 1000\n",
      "Created a chunk of size 1033, which is longer than the specified 1000\n",
      "Created a chunk of size 1891, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "pfe Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1193, which is longer than the specified 1000\n",
      "Created a chunk of size 1045, which is longer than the specified 1000\n",
      "Created a chunk of size 1937, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "pg Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1062, which is longer than the specified 1000\n",
      "Created a chunk of size 1195, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1940, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "tsla Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1199, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1013, which is longer than the specified 1000\n",
      "Created a chunk of size 1888, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 12 remaining for this stock\n",
      "Chunk 1 summarised, 11 remaining for this stock\n",
      "Chunk 2 summarised, 10 remaining for this stock\n",
      "Chunk 3 summarised, 9 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 8 remaining for this stock\n",
      "Chunk 5 summarised, 7 remaining for this stock\n",
      "Chunk 6 summarised, 6 remaining for this stock\n",
      "Chunk 7 summarised, 5 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 4 remaining for this stock\n",
      "Chunk 9 summarised, 3 remaining for this stock\n",
      "Chunk 10 summarised, 2 remaining for this stock\n",
      "Chunk 11 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "v Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1060, which is longer than the specified 1000\n",
      "Created a chunk of size 1205, which is longer than the specified 1000\n",
      "Created a chunk of size 1060, which is longer than the specified 1000\n",
      "Created a chunk of size 1946, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 summarised, 11 remaining for this stock\n",
      "Chunk 1 summarised, 10 remaining for this stock\n",
      "Chunk 2 summarised, 9 remaining for this stock\n",
      "Chunk 3 summarised, 8 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 4 summarised, 7 remaining for this stock\n",
      "Chunk 5 summarised, 6 remaining for this stock\n",
      "Chunk 6 summarised, 5 remaining for this stock\n",
      "Chunk 7 summarised, 4 remaining for this stock\n",
      "Waiting for 1 minute before processing the next 4 chunks...\n",
      "Chunk 8 summarised, 3 remaining for this stock\n",
      "Chunk 9 summarised, 2 remaining for this stock\n",
      "Chunk 10 summarised, 1 remaining for this stock\n",
      "Summarised!\n",
      "xom Done\n"
     ]
    }
   ],
   "source": [
    "# stocks_used_1 = [\"aapl\", \"amzn\", \"ba\", \"brka\"]\n",
    "# stocks_used_1 = [\"aapl\"]\n",
    "stocks_used = [\"aapl\", \"amzn\", \"ba\", \"brka\", \"googl\", \"gs\", \"jnj\", \"jpm\", \"ko\", \"mcd\", \n",
    "               \"meta\", \"ms\", \"msft\", \"nee\", \"nvda\", \"pfe\", \"pg\", \"tsla\", \"v\", \"xom\"]\n",
    "\n",
    "stocks_used_dict = dict()\n",
    "\n",
    "for stock in stocks_used:\n",
    "    doc = fitz.open(f\"{stock}_report.pdf\")\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    texts = [page.get_text(\"text\") for page in doc]\n",
    "\n",
    "    # Combine extracted text\n",
    "    full_text = \"\\n\\n\".join(texts)\n",
    "\n",
    "    # Print or use the extracted text\n",
    "    # print(full_text)\n",
    "\n",
    "    # Initialize the text splitter, and chunk the reports into more concise summaries\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000, chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    # Split text into chunks\n",
    "    texts_4k_token = text_splitter.split_text(full_text)\n",
    "\n",
    "    # Get text, table summaries\n",
    "    text_summaries = generate_text_summaries(\n",
    "        texts_4k_token, summarize_texts=True\n",
    "    )\n",
    "    stocks_used_dict[stock] = text_summaries\n",
    "    print(f\"{stock} Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Summary of Issues for Tesla Inc (0R0X-LN)\n",
      "\n",
      "The report highlights several concerning issues for Tesla Inc:\n",
      "\n",
      "* **Declining Average Score:** Tesla's average score has dropped to a 3-year low of 4, primarily due to a decline in Price Momentum. This suggests a negative outlook on the company's performance.\n",
      "* **Negative 1-Month and 3-Month Returns:** Tesla has experienced significant negative returns in the past month (-26.5%) and three months (-28.9%). This indicates a recent downward trend in the stock price.\n",
      "* **Neutral Outlook:** Despite the recent decline, Tesla's current score is still considered \"relatively in-line with the market,\" suggesting a neutral outlook. However, the declining trend raises concerns about future performance.\n",
      "* **Analyst Recommendations:** While the mean recommendation from analysts is \"Hold,\" the distribution of recommendations shows a significant number of \"Sell\" ratings. This suggests a lack of confidence in the company's future prospects.\n",
      "* **Trailing and Forward PE Ratios:** The trailing PE ratio of 1.3 and forward PE ratio of 1.0 are significantly lower than the market average, indicating a potential undervaluation of the company. However, this could also be a reflection of the current market sentiment and the company's recent performance.\n",
      "\n",
      "Overall, the report suggests that Tesla is facing several challenges, including declining performance, negative market sentiment, and a lack of confidence from analysts. While the company's long-term growth potential remains, the recent trends raise concerns about its short-term prospects. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stocks_used_dict['tsla'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stock                                          Summaries\n",
      "0   aapl  [## Summary of Issues from Apple Inc. (0R2V-LN...\n",
      "1   amzn  [## Summary of Issues for AMZN:\\n\\nThe report ...\n",
      "2     ba  [## Summary of Issues for Boeing Co (BA)\\n\\nTh...\n",
      "3   brka  [## Summary of Issues from the Berkshire Hatha...\n",
      "4  googl  [## Summary of Issues from Alphabet Inc. (GOOG...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert stocks_used_dict to a pandas DataFrame\n",
    "# Each stock symbol becomes a row, and its associated summaries become a column\n",
    "\n",
    "stocks_df = pd.DataFrame(list(stocks_used_dict.items()), columns=['Stock', 'Summaries'])\n",
    "\n",
    "# Optionally, if you want to save the DataFrame to a CSV file:\n",
    "stocks_df.to_csv('stocks_used_summaries.csv', index=False)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(stocks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/954241416931/locations/us-east4/indexes/3653589178169425920/operations/1053255723351277568\n",
      "MatchingEngineIndex created. Resource name: projects/954241416931/locations/us-east4/indexes/3653589178169425920\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/954241416931/locations/us-east4/indexes/3653589178169425920')\n"
     ]
    }
   ],
   "source": [
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings\n",
    "DIMENSIONS = 768  # Dimensions output from textembedding-gecko\n",
    "\n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"rag_index\",\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    "    description=\"RAG LangChain Index\",\n",
    "    index_update_method=\"STREAM_UPDATE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/954241416931/locations/us-east4/indexEndpoints/904634186868981760/operations/6484596873960095744\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/954241416931/locations/us-east4/indexEndpoints/904634186868981760\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/954241416931/locations/us-east4/indexEndpoints/904634186868981760')\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_INDEX_ID = \"rag_index_endpoint\"\n",
    "\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DEPLOYED_INDEX_ID,\n",
    "    description=\"RAG Index Endpoint\",\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/954241416931/locations/us-east4/indexEndpoints/904634186868981760\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/954241416931/locations/us-east4/indexEndpoints/904634186868981760/operations/5984697315321970688\n"
     ]
    }
   ],
   "source": [
    "index_endpoint = index_endpoint.deploy_index(\n",
    "    index=index, deployed_index_id=\"rag_deployed_index\"\n",
    ")\n",
    "index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "`VertexAIEmbeddings` is not fully defined; you should define `_LanguageModel`, then call `VertexAIEmbeddings.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.10/u/class-not-fully-defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming VertexAIEmbeddings and Chroma setup\u001b[39;00m\n\u001b[0;32m     10\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[0;32m     11\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 12\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39m\u001b[43mVertexAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create the in-memory docstore to store metadata (e.g., stock symbol)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m docstore \u001b[38;5;241m=\u001b[39m InMemoryStore()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_community\\embeddings\\vertexai.py:66\u001b[0m, in \u001b[0;36mVertexAIEmbeddings.__init__\u001b[1;34m(self, model_name, project, location, request_parallelism, max_retries, credentials, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# the default value would be removed after Feb-01-2024\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     64\u001b[0m ):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the sentence_transformer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     67\u001b[0m         project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[0;32m     68\u001b[0m         location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[0;32m     69\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m     70\u001b[0m         request_parallelism\u001b[38;5;241m=\u001b[39mrequest_parallelism,\n\u001b[0;32m     71\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[0;32m     72\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     74\u001b[0m     )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, _MAX_BATCH_SIZE)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pydantic\\_internal\\_mock_val_ser.py:100\u001b[0m, in \u001b[0;36mMockValSer.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# raise an AttributeError if `item` doesn't exist\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_or_ser, item)\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message, code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code)\n",
      "\u001b[1;31mPydanticUserError\u001b[0m: `VertexAIEmbeddings` is not fully defined; you should define `_LanguageModel`, then call `VertexAIEmbeddings.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.10/u/class-not-fully-defined"
     ]
    }
   ],
   "source": [
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "import uuid\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Assuming VertexAIEmbeddings and Chroma setup\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"rag_collection\",\n",
    "#     embedding_function=VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "# )\n",
    "vectorstore = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    "    gcs_bucket_name=GCS_BUCKET,\n",
    "    index_id=index.name,\n",
    "    endpoint_id=index_endpoint.name,\n",
    "    embedding=VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "    stream_update=True,\n",
    ")\n",
    "# Create the in-memory docstore to store metadata (e.g., stock symbol)\n",
    "docstore = InMemoryStore()\n",
    "\n",
    "# Define the key for document IDs (it could be stock symbols or unique IDs)\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the multi-vector retriever\n",
    "retriever_multi_vector_img = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Process the stock summaries and add to the vectorstore\n",
    "for stock, summaries in stocks_used_dict.items():\n",
    "    # Generate unique document IDs (or use stock symbols as IDs)\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    \n",
    "    # Create Document objects (with summaries and metadata)\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    \n",
    "    # Add documents (summaries) to Chroma vectorstore\n",
    "    vectorstore.add_documents(summary_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1044, which is longer than the specified 1000\n",
      "Created a chunk of size 1192, which is longer than the specified 1000\n",
      "Created a chunk of size 1049, which is longer than the specified 1000\n",
      "Created a chunk of size 1941, which is longer than the specified 1000\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     texts_4k_token \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(full_text)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Get text, table summaries\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     text_summaries \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text_summaries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts_4k_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     stocks_used_1_lst\u001b[38;5;241m.\u001b[39mappend(text_summaries)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# doc_ids = [str(uuid.uuid4()) for _ in text_summaries]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# summary_docs = [\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#     Document(page_content=s, metadata={id_key: doc_ids[i]})\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# # Raw Document Contents\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# # doc_contents = texts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mgenerate_text_summaries\u001b[1;34m(texts, summarize_texts)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m texts:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summarize_texts:\n\u001b[1;32m---> 30\u001b[0m         text_summaries \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_concurrency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m         text_summaries \u001b[38;5;241m=\u001b[39m texts\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\runnables\\base.py:3162\u001b[0m, in \u001b[0;36mRunnableSequence.batch\u001b[1;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[0;32m   3160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 3162\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m   3163\u001b[0m                 inputs,\n\u001b[0;32m   3164\u001b[0m                 [\n\u001b[0;32m   3165\u001b[0m                     \u001b[38;5;66;03m# each step a child run of the corresponding root run\u001b[39;00m\n\u001b[0;32m   3166\u001b[0m                     patch_config(\n\u001b[0;32m   3167\u001b[0m                         config, callbacks\u001b[38;5;241m=\u001b[39mrm\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3168\u001b[0m                     )\n\u001b[0;32m   3169\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m rm, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, configs)\n\u001b[0;32m   3170\u001b[0m                 ],\n\u001b[0;32m   3171\u001b[0m                 return_exceptions\u001b[38;5;241m=\u001b[39mreturn_exceptions,\n\u001b[0;32m   3172\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3173\u001b[0m             )\n\u001b[0;32m   3175\u001b[0m \u001b[38;5;66;03m# finish the root runs\u001b[39;00m\n\u001b[0;32m   3176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\runnables\\fallbacks.py:306\u001b[0m, in \u001b[0;36mRunnableWithFallbacks.batch\u001b[1;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m first_to_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m runnable \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnables:\n\u001b[1;32m--> 306\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m runnable\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    307\u001b[0m         [\u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(run_again\u001b[38;5;241m.\u001b[39mitems())],\n\u001b[0;32m    308\u001b[0m         [\n\u001b[0;32m    309\u001b[0m             \u001b[38;5;66;03m# each step a child run of the corresponding root run\u001b[39;00m\n\u001b[0;32m    310\u001b[0m             patch_config(configs[i], callbacks\u001b[38;5;241m=\u001b[39mrun_managers[i]\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[0;32m    311\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(run_again)\n\u001b[0;32m    312\u001b[0m         ],\n\u001b[0;32m    313\u001b[0m         return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     )\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (i, \u001b[38;5;28minput\u001b[39m), output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28msorted\u001b[39m(run_again\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems()), outputs):\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;167;01mBaseException\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    318\u001b[0m             output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions_to_handle\n\u001b[0;32m    319\u001b[0m         ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\llms.py:458\u001b[0m, in \u001b[0;36mBaseLLM.batch\u001b[1;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    454\u001b[0m     inputs[i : i \u001b[38;5;241m+\u001b[39m max_concurrency]\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(inputs), max_concurrency)\n\u001b[0;32m    456\u001b[0m ]\n\u001b[0;32m    457\u001b[0m config \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_concurrency\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m config]  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    459\u001b[0m     output\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches)\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    462\u001b[0m         batch,\n\u001b[0;32m    463\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig[i \u001b[38;5;241m*\u001b[39m max_concurrency : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m max_concurrency],\n\u001b[0;32m    464\u001b[0m         return_exceptions\u001b[38;5;241m=\u001b[39mreturn_exceptions,\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\llms.py:461\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    453\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    454\u001b[0m     inputs[i : i \u001b[38;5;241m+\u001b[39m max_concurrency]\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(inputs), max_concurrency)\n\u001b[0;32m    456\u001b[0m ]\n\u001b[0;32m    457\u001b[0m config \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_concurrency\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m config]  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    459\u001b[0m     output\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches)\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    462\u001b[0m         batch,\n\u001b[0;32m    463\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig[i \u001b[38;5;241m*\u001b[39m max_concurrency : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m max_concurrency],\n\u001b[0;32m    464\u001b[0m         return_exceptions\u001b[38;5;241m=\u001b[39mreturn_exceptions,\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\llms.py:438\u001b[0m, in \u001b[0;36mBaseLLM.batch\u001b[1;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_concurrency \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m         llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    439\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs],\n\u001b[0;32m    440\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39m[c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m config],\n\u001b[0;32m    441\u001b[0m             tags\u001b[38;5;241m=\u001b[39m[c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m config],\n\u001b[0;32m    442\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m[c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m config],\n\u001b[0;32m    443\u001b[0m             run_name\u001b[38;5;241m=\u001b[39m[c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m config],\n\u001b[0;32m    444\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    445\u001b[0m         )\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [g[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations]\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     ]\n\u001b[1;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    964\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 784\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    785\u001b[0m                 prompts,\n\u001b[0;32m    786\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    787\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    788\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    789\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    790\u001b[0m             )\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    793\u001b[0m         )\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_google_vertexai\\llms.py:99\u001b[0m, in \u001b[0;36mVertexAI._generate\u001b[1;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m generations: List[List[Generation]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m---> 99\u001b[0m     chat_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    100\u001b[0m         [HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)],\n\u001b[0;32m    101\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    102\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    103\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    107\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    108\u001b[0m         [\n\u001b[0;32m    109\u001b[0m             Generation(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m         ]\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_google_vertexai\\chat_models.py:1257\u001b[0m, in \u001b[0;36mChatVertexAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_gemini_model:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_non_gemini(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_gemini(\n\u001b[0;32m   1258\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1259\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   1260\u001b[0m     run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1261\u001b[0m     is_gemini\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1263\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_google_vertexai\\chat_models.py:1429\u001b[0m, in \u001b[0;36mChatVertexAI._generate_gemini\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_gemini\u001b[39m(\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1423\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1427\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1428\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request_gemini(messages\u001b[38;5;241m=\u001b[39mmessages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1429\u001b[0m     response \u001b[38;5;241m=\u001b[39m _completion_with_retry(\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_client\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1431\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m   1432\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1433\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1434\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1435\u001b[0m     )\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gemini_response_to_chat_result(response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_google_vertexai\\chat_models.py:617\u001b[0m, in \u001b[0;36m_completion_with_retry\u001b[1;34m(generation_method, max_retries, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    612\u001b[0m params \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    613\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gemini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[0;32m    616\u001b[0m )\n\u001b[1;32m--> 617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry_inner(\n\u001b[0;32m    618\u001b[0m     generation_method,\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    620\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tenacity\\__init__.py:485\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    484\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split only text documents\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        texts.append(doc)\n",
    "    return {\"texts\": texts}\n",
    "\n",
    "\n",
    "def text_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                \"You are a financial analyst tasked with providing investment advice.\\n\"\n",
    "                \"You will be given text-based data, including tables and reports.\\n\"\n",
    "                \"Use this information to provide investment advice related to the user's question.\\n\"\n",
    "                f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "                \"Text and / or tables:\\n\"\n",
    "                f\"{formatted_texts}\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "# Create RAG chain with text-only logic\n",
    "chain_multimodal_rag = (\n",
    "    {\n",
    "        \"context\": retriever_multi_vector_img | RunnableLambda(split_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(text_prompt_func)\n",
    "    | ChatVertexAI(\n",
    "        temperature=0,\n",
    "        model_name=MODEL_NAME,\n",
    "        max_output_tokens=TOKEN_LIMIT,\n",
    "    )  # Multi-modal LLM (text-only in this case)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = fitz.open(f\"tesla-stock-report.pdf\")\n",
    "# text = \"\\n\".join([page.get_text() for page in doc])\n",
    "# # print(text)\n",
    "# # Extract text from all pages\n",
    "# texts = [page.get_text(\"text\") for page in doc]\n",
    "\n",
    "# # Combine extracted text\n",
    "# full_text = \"\\n\\n\".join(texts)\n",
    "\n",
    "# # Print or use the extracted text\n",
    "# # print(full_text)\n",
    "\n",
    "# # Initialize the text splitter, and chunk the reports into more concise summaries\n",
    "# text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=2000, chunk_overlap=0\n",
    "# )\n",
    "\n",
    "# # Split text into chunks\n",
    "# texts_4k_token = text_splitter.split_text(full_text)\n",
    "\n",
    "# # Get text, table summaries\n",
    "# text_summaries = generate_text_summaries(\n",
    "#     texts_4k_token, summarize_texts=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation of RAG Model\n",
    "\n",
    "Th\n",
    "\n",
    "### Evaluation of Strengths and Weaknesses\n",
    "\n",
    "#### Strengths\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "### Potential Future Work\n",
    "\n",
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
