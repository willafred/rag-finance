{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA4265 Assignment 2: RAG Generation\n",
    "\n",
    "With the large availability of news available today from different agencies alongside the sheer number of stocks to select from today, it is increasingly difficult for investors to spend time to look through all news articles and reports about the different companies and the performance of their stocks to decide which to buy to maximise their returns. Through the controversial opinions on the performance of stocks, investors tend to rely on analyst reports in terms of scores for established metrics such as Earnings and Sentiments. \n",
    "\n",
    "Therefore, the goal of this assignment is to generate a Retriever-Augmentation-Generation (RAG) model that extracts key information about the overall performance of the stock data based on the recent windows, and providing investment advice to prospective investors about the performance of the stocks. This makes it easier for investors to make an informed decision about the investment in the stocks that have been included in the RAG model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Extraction and Preparation\n",
    "\n",
    "The following section describes the data extraction process and generation of the labelled dataframe. The data obtained was sourced from Refinitiv Workspace.\n",
    "\n",
    "The tickers used in this assignment can be summarised in the table below: \n",
    "\n",
    "| Stock Name | Ticker Symbol |\n",
    "| ---------  | ------------- |\n",
    "| Apple Inc | AAPL |\n",
    "| Amazon.com Inc | AMZN |\n",
    "| Boeing Co | BA |\n",
    "| Berkshire Hathaway Inc Class A | BRKA |\n",
    "| Google | GOOGL |\n",
    "| Goldman-Sachs | GS |\n",
    "| Johnson & Johnson | JNJ |\n",
    "| JPMorgan Chase & Co | JPM |\n",
    "| Coca-Cola Co | KO |\n",
    "| McDonald's Corp | MCD |\n",
    "| Meta Platforms Inc | META |\n",
    "| Morgan Stanley | MS |\n",
    "| Microsoft Corp | MSFT |\n",
    "| NextEra Energy Inc | NEE |\n",
    "| NVIDIA Corp | NVDA |\n",
    "| Pfizer Inc | PFE |\n",
    "| Procter & Gamble Co | PG |\n",
    "| Tesla Inc | TSLA |\n",
    "| Visa Inc | V |\n",
    "| Exxon Mobil Corp | XOM |\n",
    "\n",
    "### Feature 1: Summarisation of Analytic Reports\n",
    "\n",
    "As the documents included in the dataset are relatively long, the use of LLMs was used to summarise the different chunks, and these summarised chunks will then be used for embedding and subsequently to answer the query.\n",
    "\n",
    "### Feature 2: Chunking of Documents\n",
    "\n",
    "To facilitate the separation of documents into distinct chunks, CharacterTextSplitter function was utilised, with an overlap of 100 characters so as to ensure the preservation of context between chunks. Therefore, this enables better understanding of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_used = [\"aapl\", \"amzn\", \"ba\", \"brka\", \"googl\", \"gs\", \"jnj\", \"jpm\", \"ko\", \"mcd\", \n",
    "               \"meta\", \"ms\", \"msft\", \"nee\", \"nvda\", \"pfe\", \"pg\", \"tsla\", \"v\", \"xom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_google_vertexai import (\n",
    "    ChatVertexAI,\n",
    "    VectorSearchVectorStore,\n",
    "    VertexAI,\n",
    "    VertexAIEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from google.cloud import aiplatform\n",
    "import fitz  # pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for VertexAI\n",
    "\n",
    "PROJECT_ID = \"PROJECT-ID\"\n",
    "LOCATION = \"LOCATION\"\n",
    "\n",
    "# For Vector Search Staging\n",
    "GCS_BUCKET = \"BUCKET-ID\"\n",
    "GCS_BUCKET_URI = f\"gs://{GCS_BUCKET}\"\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=GCS_BUCKET_URI)\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "GEMINI_OUTPUT_TOKEN_LIMIT = 8192\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-004\"\n",
    "EMBEDDING_TOKEN_LIMIT = 2048\n",
    "\n",
    "TOKEN_LIMIT = min(GEMINI_OUTPUT_TOKEN_LIMIT, EMBEDDING_TOKEN_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise text using VertexAPI\n",
    "\n",
    "import time\n",
    "def generate_text_summaries(\n",
    "    texts: list[str], summarize_texts: bool = False\n",
    ") -> tuple[list, list]:\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Summarise the issues stemming for the report provided. The report is as shown: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "    empty_response = RunnableLambda(\n",
    "        lambda x: AIMessage(content=\"Error processing document\")\n",
    "    )\n",
    "    # Text summary chain\n",
    "    model = VertexAI(\n",
    "        temperature=0, model_name=MODEL_NAME, max_output_tokens=TOKEN_LIMIT\n",
    "    ).with_fallbacks([empty_response])\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "\n",
    "    if texts:\n",
    "        for i in range(len(texts)):\n",
    "            text = texts[i]\n",
    "            if summarize_texts:\n",
    "                # Summarize the current text chunk\n",
    "                summary = summarize_chain.invoke({\"element\": text})\n",
    "                text_summaries.append(summary)\n",
    "            else:\n",
    "                text_summaries.append(text)\n",
    "            print(f\"Chunk {i} summarised, {len(texts)-i} remaining for this stock\")\n",
    "            # Wait for 1 minute after every 3 chunks\n",
    "            if (i + 1) % 4 == 0 and i != len(texts) - 1:\n",
    "                print(\"Waiting for 1 minute before processing the next 4 chunks...\")\n",
    "                time.sleep(60)  # Delay for 1 minute after every 3 chunks\n",
    "    print(\"Summarised!\")\n",
    "    return text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_used = [\"aapl\", \"amzn\", \"ba\", \"brka\", \"googl\", \"gs\", \"jnj\", \"jpm\", \"ko\", \"mcd\", \n",
    "               \"meta\", \"ms\", \"msft\", \"nee\", \"nvda\", \"pfe\", \"pg\", \"tsla\", \"v\", \"xom\"]\n",
    "\n",
    "stocks_used_dict = dict()\n",
    "\n",
    "for stock in stocks_used:\n",
    "    doc = fitz.open(f\"{stock}_report.pdf\")\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    texts = [page.get_text(\"text\") for page in doc]\n",
    "\n",
    "    # Combine extracted text\n",
    "    full_text = \"\\n\\n\".join(texts)\n",
    "\n",
    "    # Initialize the text splitter, and chunk the reports into more concise summaries\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000, chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    # Split text into chunks\n",
    "    texts_4k_token = text_splitter.split_text(full_text)\n",
    "\n",
    "    # Get text summaries from report\n",
    "    text_summaries = generate_text_summaries(\n",
    "        texts_4k_token, summarize_texts=True\n",
    "    )\n",
    "    stocks_used_dict[stock] = text_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building of RAG Model\n",
    "\n",
    "For each ticker's analytics report, the summarised chunks are stored in a list. These lists will then be collated into a dictionary format, where the stock tickers act as the keys of the dictionary for ease of identification. The text summarising function is coded into a loop across all the ticker reports. Following this, the RAG model was built based on the steps which will be described in greater depth below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stock                                          Summaries\n",
      "0   aapl  [## Summary of Issues from Apple Inc. (0R2V-LN...\n",
      "1   amzn  [## Summary of Issues for AMZN:\\n\\nThe report ...\n",
      "2     ba  [## Summary of Issues for Boeing Co (BA)\\n\\nTh...\n",
      "3   brka  [## Summary of Issues from the Berkshire Hatha...\n",
      "4  googl  [## Summary of Issues from Alphabet Inc. (GOOG...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert stocks_used_dict to a pandas DataFrame, where each stock symbol becomes a row, and its associated summaries become a column\n",
    "stocks_df = pd.DataFrame(list(stocks_used_dict.items()), columns=['Stock', 'Summaries'])\n",
    "\n",
    "# Saving\n",
    "stocks_df.to_csv('stocks_used_summaries.csv', index=False)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(stocks_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: RAG Model Set-Up\n",
    "\n",
    "This section discusses the various methodology involved in the generation of the RAG Model to answer queries based on the impressions of various stocks. To generate the RAG Model, VertexAI was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Retrieval of Data\n",
    "\n",
    "In this stage, the following steps are applied:\n",
    "\n",
    "1. Checking for duplicates in the summaries for each stock to ensure that the results will not be biased to particular chunks.\n",
    "2. VertexAI vector search index & endpoint is deployed for ease of access to the embedding vectors.\n",
    "3. Creation of Retriever was done with the help of VectorSearchVectorStore with the Vector Search Index ID and Endpoint ID, and the embedding model as textembedding-gecko. This allows the querying of the vector index to retrieve documents that are semantically similar to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "import uuid\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_google_vertexai import (\n",
    "    ChatVertexAI,\n",
    "    VectorSearchVectorStore,\n",
    "    VertexAI,\n",
    "    VertexAIEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from google.cloud import aiplatform\n",
    "import fitz  # pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "stocks_used_dict = pd.read_csv('stocks_used_summaries.csv')\n",
    "\n",
    "# Convert the string representation of a list back to an actual list\n",
    "stocks_used_dict['Summaries'] = stocks_used_dict['Summaries'].apply(ast.literal_eval)\n",
    "stocks_used_dict = stocks_used_dict.set_index('Stock')['Summaries'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl stock has no duplicate lengths.\n",
      "amzn stock has no duplicate lengths.\n",
      "ba stock has no duplicate lengths.\n",
      "brka stock has no duplicate lengths.\n",
      "googl stock has no duplicate lengths.\n",
      "gs stock has no duplicate lengths.\n",
      "jnj stock has no duplicate lengths.\n",
      "jpm stock has no duplicate lengths.\n",
      "ko stock has summaries with duplicate lengths: {1641}\n",
      "mcd stock has no duplicate lengths.\n",
      "meta stock has no duplicate lengths.\n",
      "ms stock has no duplicate lengths.\n",
      "msft stock has no duplicate lengths.\n",
      "nee stock has no duplicate lengths.\n",
      "nvda stock has no duplicate lengths.\n",
      "pfe stock has no duplicate lengths.\n",
      "pg stock has no duplicate lengths.\n",
      "tsla stock has no duplicate lengths.\n",
      "v stock has no duplicate lengths.\n",
      "xom stock has no duplicate lengths.\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates in summaries; after checking, KO does not have duplicates (even though the summaries are of the same length)\n",
    "from collections import Counter\n",
    "\n",
    "for stock, summaries in stocks_used_dict.items():\n",
    "    lengths = [len(s) for s in summaries]  # List of lengths of each summary\n",
    "    length_counts = Counter(lengths)  # Count occurrences of each length\n",
    "    duplicates = {length for length, count in length_counts.items() if count > 1}  # Set of duplicate lengths\n",
    "    if duplicates:\n",
    "        print(f\"{stock} stock has summaries with duplicate lengths: {duplicates}\")\n",
    "    else:\n",
    "        print(f\"{stock} stock has no duplicate lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/954241416931/locations/us-central1/indexes/2117250376771043328/operations/1220752679026819072\n",
      "MatchingEngineIndex created. Resource name: projects/954241416931/locations/us-central1/indexes/2117250376771043328\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/954241416931/locations/us-central1/indexes/2117250376771043328')\n",
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/954241416931/locations/us-central1/indexEndpoints/5350764540478881792/operations/3368547488817479680\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/954241416931/locations/us-central1/indexEndpoints/5350764540478881792\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/954241416931/locations/us-central1/indexEndpoints/5350764540478881792')\n",
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/954241416931/locations/us-central1/indexEndpoints/5350764540478881792\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/954241416931/locations/us-central1/indexEndpoints/5350764540478881792/operations/4267015614477893632\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/954241416931/locations/us-central1/indexEndpoints/5350764540478881792\n"
     ]
    }
   ],
   "source": [
    "# Creation of endpoints\n",
    "DIMENSIONS = 768  # Dimensions output from textembedding-gecko\n",
    "\n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"rag_index\",\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    "    description=\"RAG LangChain Index\",\n",
    "    index_update_method=\"STREAM_UPDATE\",\n",
    ")\n",
    "\n",
    "DEPLOYED_INDEX_ID = \"rag_index_endpoint\"\n",
    "\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DEPLOYED_INDEX_ID,\n",
    "    description=\"RAG Index Endpoint\",\n",
    "    public_endpoint_enabled=True,\n",
    ")\n",
    "\n",
    "index_endpoint = index_endpoint.deploy_index(\n",
    "    index=index, deployed_index_id=\"rag_deployed_index1\"\n",
    ")\n",
    "# index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "import uuid\n",
    "from langchain.schema import Document\n",
    "\n",
    "vectorstore = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    "    gcs_bucket_name=GCS_BUCKET,\n",
    "    index_id=index.name,\n",
    "    endpoint_id=index_endpoint.name,\n",
    "    embedding=VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "    stream_update=True,\n",
    ")\n",
    "\n",
    "# Create the in-memory docstore to store metadata (e.g., stock symbol)\n",
    "docstore = InMemoryStore()\n",
    "\n",
    "# Define the key for document IDs (it could be stock symbols or unique IDs)\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Process the stock summaries and add to the vectorstore\n",
    "for stock, summaries in stocks_used_dict.items():\n",
    "    # Generate unique document IDs (or use stock symbols as IDs)\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    \n",
    "    # Create Document objects (with summaries and metadata)\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    \n",
    "    # Add documents (summaries) to Chroma vectorstore\n",
    "    vectorstore.add_documents(summary_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Introduction of Query, and Similarity Search\n",
    "\n",
    "In this stage, given a query, the similarity_search function was used to embed the query, and find the chunks that are semantically the closest to the embedded query. Subsequently, these chunks will function as the context, from which it will be fed into the ChatVertexAI LLM to construct a response to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain with text-only logic\n",
    "def chain_rag(query, num_k, temp = 0):\n",
    "    docs = vectorstore.similarity_search(query, k=num_k)\n",
    "    all_texts = []\n",
    "    for doc in docs:\n",
    "        all_texts.append(doc.page_content)\n",
    "    formatted_texts = \"\\n\".join(all_texts)\n",
    "    \n",
    "    model = ChatVertexAI(\n",
    "        temperature=temp,\n",
    "        model_name=MODEL_NAME,\n",
    "        max_output_tokens=TOKEN_LIMIT,\n",
    "    )\n",
    "    # Prepare the message to send to the model\n",
    "    message = (\n",
    "        \"You are a financial analyst tasked with providing investment advice.\\n\"\n",
    "        \"You will be given text-based data.\\n\"\n",
    "        \"Use this information to provide investment advice related to the user's question.\\n\"\n",
    "        f\"User-provided question: {query}\\n\\n\"\n",
    "        \"Text:\\n\"\n",
    "        f\"{formatted_texts}\\n\\n\"\n",
    "        \"Your response should include:\\n\"\n",
    "        \"- A summary of relevant information from the provided text.\\n\"\n",
    "        \"- An analysis of key financial indicators or trends.\\n\"\n",
    "        \"- A conclusion based on the evidence, explicitly stating how the data supports your recommendation.\\n\"\n",
    "        \"- Citations or references to specific data points where applicable.\"\n",
    "        \"You do not need to repeat the answer twice.\"\n",
    "    )\n",
    "    \n",
    "    # Send the message to the model and get the response\n",
    "    response = model([HumanMessage(content=message)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation of RAG Model\n",
    "\n",
    "To evaluate the model, a groundedness check was done to check the validity of the answers itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Method 1: Groundedness Evaluation\n",
    "\n",
    "Following the preparation of the RAG chain to generate based on the queries, checking that the RAG model works was then done by means of a groundedness check to ensure that the answers are not randomly generated. This step is critical in financial applications, as it prevents hallucinations and enhances trust in the generated advice. Through this process, we can also confirm that model outputs align with factual data rather than simply speculations.\n",
    "\n",
    "In this project, I tested it using the methods listed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type 1: Hallucinations\n",
    "In this type, I test for incorrect or fabricated answers, as TSLA does NOT pay dividends.\n",
    "\n",
    "In this example, hallucinations are not observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided report, Tesla (TSLA) does not currently pay a dividend. [2] This means its dividend yield is **0%**. [2] The report highlights that Tesla is one of seven companies in its industry group that does not pay a dividend. [2] Additionally, the report lacks information on dividend metrics such as payout, coverage, and yield, making it difficult to assess the company's dividend policy and potential future dividend payments. [5] \n",
      "\n",
      "Therefore, based on the available information, Tesla does not offer a dividend yield, and there is no indication of future dividend payments. Investors seeking dividend income should consider other investment options. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_1 = chain_rag(\"What is Tesla’s (TSLA) dividend yield?\", num_k=1)\n",
    "print(answer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type 2: Comparative Analysis\n",
    "This example tests the reasoning and logic behind the retrieval of the documents. This checks if the RAG model is able to synthesise and process the material thoroughly to make a decision.\n",
    "\n",
    "In this case, a decision was made with reasonable evidence from the analyst reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Investment Advice: Pfizer vs. Berkshire Hathaway\n",
      "\n",
      "The question asks which stock, Pfizer (PFE) or Berkshire Hathaway (BRK.A), investors are more optimistic about. While both companies have positive outlooks, a closer examination of the provided data reveals a more nuanced picture.\n",
      "\n",
      "**Pfizer (PFE):**\n",
      "\n",
      "* **Positive Earnings:** Pfizer boasts a strong earnings rating of 9, significantly higher than the Pharmaceuticals industry average of 5.3. [Text: Pfizer Inc. (PFE) Earnings Report Summary]\n",
      "* **Consistent Earnings Surprises:** Pfizer has consistently exceeded earnings expectations over the past four quarters. [Text: Pfizer Inc. (PFE) Earnings Report Summary]\n",
      "* **Positive Estimate Revisions:** Pfizer's current quarter consensus estimate has increased by 5.3% over the past 90 days. [Text: Pfizer Inc. (PFE) Earnings Report Summary]\n",
      "* **Analyst Price Target:** Analysts predict a 12-month price target of $30.60, representing a 16.6% increase from the current price. [Text: Pfizer Inc. (PFE) Earnings Report Summary]\n",
      "\n",
      "**However, several concerns exist:**\n",
      "\n",
      "* **Declining Average Score Trend:** Pfizer's average score has been declining over the past year, suggesting potential volatility. [Text: Summary of Issues from Pfizer Inc. (PFE) Report]\n",
      "* **Lagging Peer Performance:** Pfizer's score has been consistently lower than its peers, indicating a potential competitive disadvantage. [Text: Summary of Issues from Pfizer Inc. (PFE) Report]\n",
      "* **Negative Price and Volume Trends:** Pfizer's stock price has been declining over the past year and five years, suggesting investor concerns about the company's future prospects. [Text: Summary of Issues from Pfizer Inc. (PFE) Report]\n",
      "* **Mixed Analyst Recommendations:** While the mean recommendation is \"Hold,\" a significant number of analysts recommend \"Sell,\" indicating a lack of strong confidence in the company's future performance. [Text: Summary of Issues from Pfizer Inc. (PFE) Report]\n",
      "\n",
      "**Berkshire Hathaway (BRK.A):**\n",
      "\n",
      "* **Improved Earnings Score:** BRK.A's earnings score has improved significantly in the past week, although it remains below the average for the Consumer Goods Conglomerates sector. [Text: Summary of Issues for Berkshire Hathaway Inc (BRK'A)]\n",
      "* **Positive Earnings Surprises:** BRK.A has a history of positive earnings surprises, with 3 positive surprises in the last 4 quarters. [Text: Summary of Issues for Berkshire Hathaway Inc (BRK'A)]\n",
      "* **Analyst Price Target:** Analysts are generally bullish on BRK.A's future price, with a mean price target of 784,000.00 USD. [Text: Summary of Issues for Berkshire Hathaway Inc (BRK'A)]\n",
      "\n",
      "**However, concerns exist:**\n",
      "\n",
      "* **Decreased Estimate Revisions:** The consensus estimate for BRK.A's current quarter earnings has decreased over the past 90 days, indicating potential concerns about future performance. [Text: Summary of Issues for Berkshire Hathaway Inc (BRK'A)]\n",
      "* **Wide Price Target Range:** The price target range for BRK.A is wide, suggesting uncertainty about its future performance. [Text: Summary of Issues for Berkshire Hathaway Inc (BRK'A)]\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Based on the provided data, investors appear to be more optimistic about **Berkshire Hathaway (BRK.A)**. While Pfizer has strong earnings and positive estimate revisions, its declining average score trend, lagging peer performance, negative price and volume trends, and mixed analyst recommendations raise concerns about its future prospects. BRK.A, on the other hand, has a history of positive earnings surprises and a bullish analyst outlook, despite recent concerns about estimate revisions and the wide price target range. \n",
      "\n",
      "**Recommendation:** Investors seeking a more optimistic outlook may consider BRK.A over PFE, although further research and due diligence are recommended before making any investment decisions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_2 = chain_rag(\"It is noted that Pfizer and BRK.A have among the highest positive outlooks. But which stock are investors more optimistic about?\", num_k=5)\n",
    "print(answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type 3: Giving Logical Advice\n",
    "This example evaluates the ability for the model to assess the risks associated with the stock, and make a decision accordingly.\n",
    "\n",
    "In this case, the evaluation of the Amazon stock is well-balanced, with proper evaluation of the stock risk profile with benchmark stocks such as S&P 500, thereby indicating a decent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Investing in Amazon: A Risky Yet Potentially Rewarding Proposition\n",
      "\n",
      "Amazon (AMZN) is a company with a strong track record of growth and innovation, but its stock comes with significant risks. The provided report highlights several key factors to consider:\n",
      "\n",
      "**High Risk Profile:**\n",
      "\n",
      "* **High Correlation with S&P 500:** AMZN's performance closely mirrors the broader market, offering limited diversification benefits (Report, \"Summary of Issues\").\n",
      "* **Volatility and Beta:** AMZN is more volatile than the S&P 500, experiencing larger drops during market downturns (Report, \"Summary of Issues\").\n",
      "* **Risk Rating:** AMZN has a higher risk rating (10) than the S&P 500 average (8.8), indicating a higher level of risk associated with the stock (Report, \"Summary of Issues\").\n",
      "\n",
      "**Recent Performance Concerns:**\n",
      "\n",
      "* **Declining Average Score:** AMZN's average score has dropped from 9 to 8, attributed to declining price momentum and insider trading scores (Report, \"Summary of Issues for AMZN\").\n",
      "* **Negative Recent Returns:** AMZN has experienced negative returns in the past month (-16.0%) and three months (-11.6%), suggesting potential investor concerns (Report, \"Summary of Issues for AMZN\").\n",
      "* **Mixed Analyst Sentiment:** While the mean recommendation is \"Buy,\" the distribution of recommendations is skewed towards \"Hold\" and \"Sell,\" indicating a mixed outlook (Report, \"Summary of Issues for AMZN\").\n",
      "\n",
      "**High Valuation:**\n",
      "\n",
      "* **Trailing PE Ratio:** AMZN's trailing PE ratio of 36.3 is significantly higher than the market average, suggesting a high valuation (Report, \"Summary of Issues for AMZN\").\n",
      "* **Forward PE Ratio:** The forward PE ratio of 32.1 also indicates a high valuation, making the stock vulnerable to a correction if the company fails to meet expectations (Report, \"Summary of Issues for AMZN\").\n",
      "\n",
      "**Lack of Dividend:**\n",
      "\n",
      "* AMZN does not pay a dividend, which may be a concern for investors seeking income (Report, \"Summary of Issues for AMZN\").\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "While Amazon remains a dominant player in e-commerce and cloud computing, its high risk profile, recent performance concerns, and high valuation present significant challenges for investors. The lack of a dividend further adds to the risk. \n",
      "\n",
      "**Recommendation:**\n",
      "\n",
      "Given the current market conditions and the company's high risk profile, investors with a high risk tolerance and a long-term investment horizon may consider a small position in AMZN. However, investors seeking a more conservative approach or those seeking income should avoid investing in AMZN at this time. \n",
      "\n",
      "**Disclaimer:** This is not financial advice. Please consult with a qualified financial advisor before making any investment decisions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_3 = chain_rag(\"What are the advantages and disadvantages of investing in Amazon?\", num_k=5)\n",
    "print(answer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Method 2: LLM Checking\n",
    "\n",
    "Apart from using groundedness check to ensure the validity of the results, I also decided to test the results using VertexAI LLM as well to evaluate the responses of the answers on a scale of 1 to 100. I conducted this using a loop with the specifications of k = [4, 6, 8] and temperature = [0, 0.2, 0.4]. Based on the responses, an overall table for each question will be constructed to compare the quality of responses based on the question to decide the best-performing parameters. This is in hopes of seeing if there are general patterns in parameters that yield better quality responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_k = [4,6,8]\n",
    "temp_lst = [0,0.2,0.4]\n",
    "answer1_lst = []\n",
    "answer2_lst = []\n",
    "answer3_lst = []\n",
    "i = 0\n",
    "for k_trial in num_k:\n",
    "    for temp_val in temp_lst:\n",
    "        ans1 = chain_rag(\"Should I invest in JPM now?\", num_k=k_trial, temp=temp_val)\n",
    "        answer1_lst.append(ans1)\n",
    "        ans2 = chain_rag(\"Is TSLA or NEE better positioned for the renewable energy sector?\", num_k=k_trial, temp=temp_val)\n",
    "        answer2_lst.append(ans2)\n",
    "        ans3 = chain_rag(\"Is NVDA a good long-term investment based on recent reports?\", num_k=k_trial, temp=temp_val)\n",
    "        answer3_lst.append(ans3)\n",
    "        if (i + 1) % 2 == 0:\n",
    "                time.sleep(60)  # Delay for 1 minute after every 6 times\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_evaluator(query, ans_list):\n",
    "    score_list = []\n",
    "    model = ChatVertexAI(\n",
    "        temperature=0,\n",
    "        model_name=MODEL_NAME,\n",
    "        max_output_tokens=TOKEN_LIMIT,\n",
    "    )\n",
    "    for i in range(len(ans_list)):\n",
    "        ans = ans_list[i]\n",
    "        # Prepare the message to send to the model\n",
    "        message = (\n",
    "            \"You are a financial analyst tasked with evaluating investment advice.\\n\"\n",
    "            \"Use this information to evaluate to the user's question.\\n\"\n",
    "            f\"User-provided question: {query}\\n\\n\"\n",
    "            \"Text:\\n\"\n",
    "            f\"{ans}\\n\\n\"\n",
    "            \"Please provide a score out of 100 that reflects the overall quality of the answers.\\n\"\n",
    "            \"The score should consider clarity, relevance, depth, and accuracy.\"\n",
    "            \"You do not need an explanation, and you do not need to repeat the answer twice.\"\n",
    "        )\n",
    "    \n",
    "        # Send the message to the model and get the response\n",
    "        response = model([HumanMessage(content=message)])\n",
    "        score_list.append(response.content)\n",
    "        # print(response.content)\n",
    "        if (i + 1) % 3 == 0:\n",
    "            print(\"Waiting for 1 minute before processing the next few chunks...\")\n",
    "            time.sleep(60)  # Delay for 1 minute after every few calls\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lst1 = llm_evaluator(\"Should I invest in JPM now?\", answer1_lst)\n",
    "score_lst2 = llm_evaluator(\"Is TSLA or NEE better positioned for the renewable energy sector?\", answer2_lst)\n",
    "score_lst3 = llm_evaluator(\"Is NVDA a good long-term investment based on recent reports?\", answer3_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from Evaluation Method 2\n",
    "The following describes the results from the use of the VertexAI LLM to evaluate the answers produced from the RAG Models:\n",
    "\n",
    "##### Question 1\n",
    "| k    | 0    | 0.2 |  0.4    |\n",
    "| :--: | :--: | :-: | :-----: |\n",
    "| 0 |   85   |  85   |   85      |\n",
    "| 0.2 |   85  |  85  |    85    |\n",
    "| 0.4 |   85  |  85  |   85     |\n",
    "\n",
    "##### Question 2\n",
    "| k    | 0    | 0.2 |  0.4    |\n",
    "| :--: | :--: | :-: | :-----: |\n",
    "| 0 |   85   |    85 |  85       |\n",
    "| 0.2 |   90  |  90  |    85    |\n",
    "| 0.4 |   90  |  85  |   90     |\n",
    "\n",
    "##### Question 3\n",
    "| k    | 0    | 0.2 |  0.4    |\n",
    "| :--: | :--: | :-: | :-----: |\n",
    "| 0 |   90   |   85  |    85     |\n",
    "| 0.2 |   85  |  85  |    90    |\n",
    "| 0.4 |   85  |  90  |   85     |\n",
    "\n",
    "Based on the results, no clear pattern is observed as temperature and the k varies for each question. Therefore, for this particular RAG model, there does not appear to be optimal parameters for any given question - rather, the best solution (based on the tested parameters) will vary based on each prompt posed to the model. More research can be done in this field for more parameters to determine if there are optimum parameters for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Overall Evaluation of Model and Conclusion\n",
    "\n",
    "Through the usage of 2 different types of evaluation methods, the quality of the responses produced by the RAG model was relatively similar across the parameters tested, and the answers produced have been shown to be able to provide logical advice, conduct comparative analyses between stocks and companies, and also appear not to have any hallucinations. \n",
    "\n",
    "#### Strengths of Model\n",
    "Firstly, from retrieving relevant documents using vector search to using LLMs such as ChatVertexAI, VertexAI facilitates the entire RAG process. This is extremely key for RAG, as this allows both efficient document retrieval and the ability to generate relevant, context-rich responses to be achieved. The advantages mentioned above is also something commendable about the model built.\n",
    "\n",
    "Next, it also integrates well with Cloud Storage, thereby enabling smooth data pipeline management and faster training cycles for the models used.\n",
    "\n",
    "#### Weaknesses of Model\n",
    "Given the strengths of using VertexAI, there also comes weaknesses of the system. By requiring frequent retrieval operations, RAG models built from Vertex AI can lead to high compute costs, particularly when dealing with large document collections and frequent queries. The cost of both vector search during the retrieval of documents and the generation of text responses can accumulate very quickly. Additionally, given that it will there will be large volumes of queries in subsequent work, the costs of using this service can be very high as well.\n",
    "\n",
    "### Potential Future Work\n",
    "Firstly, as only texts were considered in the RAG model built in this assignment, multi-modal RAG models could be considered since these models will be able to take in images and process the images and graphs to provide a better view of the performance of the stocks. Therefore, this improves the answers from the model.\n",
    "\n",
    "Next, for future works, beyond merely providing an answer or advice to investors, this RAG model has the potential to be adapted to forecast risk factors into the financial sector through the incorporation of more diverse financial indicators. Hence, better predictions can be generated as well.\n",
    "\n",
    "Finally, the measurement of performance of the RAG model can also be improved in terms of comparing the performance. Beyond simply generating and showing evidence, perhaps additional techniques such as the measurement of importance of particular phrases can be done so as to provide greater insight as to why the answer was provided by the RAG model.\n",
    "\n",
    "### Conclusion\n",
    "All in all, this model shows great potential in providing advice to prospective investors about different stocks through the summarisation of the reports in point form, and also through the comparative analysis between stocks. It also does not produce hallucinations which may mislead investors. However, more can be done to enhance the model in the long term to help investors in greater ways such as the use of multi-modal RAG models, and adaptations to achieve other functions investors intend to see such as performance forecasts for the various stocks based on the recent analyst reports."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
